<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Qianfan-VL: Domain-Enhanced Multimodal Models | Baidu AI Cloud</title>
    <meta name="description" content="Qianfan-VL series: A comprehensive family of vision-language models with domain-specific excellence">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <link rel="stylesheet" href="style.css">
    
    <style>
        /* Override Prism theme for dark code blocks */
        pre[class*="language-"],
        code[class*="language-"] {
            background: #1e1e1e !important;
            color: #d4d4d4 !important;
            text-shadow: none !important;
            -webkit-font-smoothing: auto !important;
            -moz-osx-font-smoothing: auto !important;
        }
        
        :not(pre) > code[class*="language-"],
        pre[class*="language-"] {
            background: #1e1e1e !important;
            text-shadow: none !important;
        }
        
        /* Ensure all code blocks have dark background */
        pre {
            background: #1e1e1e !important;
            border: 1px solid #3e3e42 !important;
            text-shadow: none !important;
        }
        
        pre code {
            background: transparent !important;
            color: #d4d4d4 !important;
            text-shadow: none !important;
            font-weight: normal !important;
        }
        
        /* Remove any text shadows and backgrounds from tokens */
        .token {
            text-shadow: none !important;
            background: transparent !important;
        }
        
        /* Ensure operators have no background */
        .token.operator {
            background: transparent !important;
        }
    </style>
    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // Tab switching functionality
            const tabs = document.querySelectorAll('.capability-tab');
            const panels = document.querySelectorAll('.capability-panel');
            
            tabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    const targetTab = tab.dataset.tab;
                    
                    // Remove active class from all tabs and panels
                    tabs.forEach(t => t.classList.remove('active'));
                    panels.forEach(p => p.classList.remove('active'));
                    
                    // Add active class to clicked tab and corresponding panel
                    tab.classList.add('active');
                    document.getElementById(targetTab + '-panel').classList.add('active');
                });
            });
            
            // Code tab switching functionality
            const codeTabs = document.querySelectorAll('.code-tab');
            const codePanels = document.querySelectorAll('.code-panel');
            
            codeTabs.forEach(tab => {
                tab.addEventListener('click', () => {
                    const targetTab = tab.dataset.tab;
                    
                    // Remove active class from all tabs and panels
                    codeTabs.forEach(t => t.classList.remove('active'));
                    codePanels.forEach(p => p.classList.remove('active'));
                    
                    // Add active class to clicked tab and corresponding panel
                    tab.classList.add('active');
                    document.getElementById(targetTab + '-panel').classList.add('active');
                });
            });
            
            // Carousel functionality
            const carousels = document.querySelectorAll('.carousel-container');
            
            carousels.forEach(carousel => {
                const track = carousel.querySelector('.carousel-track');
                const cards = track.querySelectorAll('.capability-card');
                const prevBtn = carousel.querySelector('.carousel-prev');
                const nextBtn = carousel.querySelector('.carousel-next');
                
                let currentIndex = 0;
                const cardWidth = 520; // Card width (500px) + gap (20px)
                const visibleCards = Math.floor(carousel.offsetWidth / cardWidth);
                const maxIndex = Math.max(0, cards.length - visibleCards);
                
                function updateCarousel() {
                    const offset = -currentIndex * cardWidth;
                    track.style.transform = `translateX(${offset}px)`;
                    
                    // Update button states
                    prevBtn.disabled = currentIndex === 0;
                    nextBtn.disabled = currentIndex >= maxIndex;
                }
                
                prevBtn.addEventListener('click', () => {
                    if (currentIndex > 0) {
                        currentIndex--;
                        updateCarousel();
                    }
                });
                
                nextBtn.addEventListener('click', () => {
                    if (currentIndex < maxIndex) {
                        currentIndex++;
                        updateCarousel();
                    }
                });
                
                // Initialize carousel
                updateCarousel();
                
                // Update on window resize
                window.addEventListener('resize', () => {
                    const newVisibleCards = Math.floor(carousel.offsetWidth / cardWidth);
                    const newMaxIndex = Math.max(0, cards.length - newVisibleCards);
                    if (currentIndex > newMaxIndex) {
                        currentIndex = newMaxIndex;
                    }
                    updateCarousel();
                });
            });
        });
    </script>
</head>
<body>
    <header class="site-header">
        <div class="container">
            <div class="header-content">
                <h1 class="site-title">Qianfan-VL Technical Documentation</h1>
                <nav class="main-nav">
                    <a href="#overview">Overview</a>
                    <a href="#innovations">Innovations</a>
                    <a href="#architecture">Architecture</a>
                    <a href="#training">Training</a>
                    <a href="#benchmarks">Benchmarks</a>
                    <a href="#applications">Model Capabilities</a>
                    <a href="#api">API</a>
                </nav>
            </div>
        </div>
    </header>

    <main class="main-content">
        <div class="container">
            <div class="hero-section">
                <h2 class="hero-title">Qianfan-VL: A Domain-Enhanced Multimodal Model Series</h2>
                <p class="hero-subtitle">Bridging the Gap Between General-Purpose Capabilities and Domain-Specific Excellence</p>
                <p class="hero-meta">3B to 70B Parameters | Released August 2025 | Baidu AI Cloud</p>
            </div>

            <section id="overview" class="section">
                <h2>Introduction</h2>
                <p>
                    In the rapidly evolving landscape of multimodal large language models, Baidu AI Cloud introduces the <strong>Qianfan-VL series</strong> - 
                    a comprehensive family of vision-language models designed to bridge the gap between general-purpose capabilities and domain-specific excellence. 
                    With model sizes ranging from 3B to 70B parameters, Qianfan-VL represents a significant advancement in making multimodal AI both powerful 
                    and practical for enterprise applications.
                </p>
                
                <p>
                    The multimodal understanding capability has become essential for enterprise intelligence transformation. While the past year has witnessed 
                    remarkable progress in multimodal models, several challenges continue to hinder widespread deployment. Qianfan-VL addresses these challenges 
                    head-on with innovative solutions in cross-modal alignment, domain enhancement, and efficient training on Kunlun chips.
                </p>

                <h3>Model Family</h3>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <div class="metric-value">Qianfan-VL-3B</div>
                        <div class="metric-label">Lightweight deployment for edge and mobile</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">Qianfan-VL-8B</div>
                        <div class="metric-label">Balanced performance for enterprise</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">Qianfan-VL-70B</div>
                        <div class="metric-label">Maximum capability for complex tasks</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">3.5T Tokens</div>
                        <div class="metric-label">Multimodal pre-training data</div>
                    </div>
                </div>
            </section>

            <section id="innovations" class="section">
                <h2>Key Innovations</h2>
                
                <h3>1. Domain-Enhanced Training Paradigm</h3>
                <p>
                    Unlike traditional multimodal models that focus solely on general capabilities, Qianfan-VL introduces a unique domain-enhanced 
                    training approach. The model specifically optimizes for high-frequency enterprise tasks while maintaining strong general performance.
                </p>
                
                <div class="feature-card">
                    <h4>Training Approach Components</h4>
                    <ul>
                        <li><strong>Continuous Multimodal Pre-training:</strong> Starting with 3.5T image-text tokens for comprehensive knowledge injection</li>
                        <li><strong>Domain-Enhanced Data Synthesis:</strong> Targeted enhancement for OCR, document understanding, K-12 education, and more</li>
                        <li><strong>Task-Specific Scenario Expansion:</strong> Systematic coverage of diverse application scenarios within each domain</li>
                    </ul>
                </div>

                <h3>2. Three-Stage Training Strategy</h3>
                <p>Qianfan-VL employs a sophisticated three-stage training process:</p>
                
                <ol class="training-stages">
                    <li>
                        <strong>Stage 1: Cross-Modal Alignment</strong>
                        <p>• Establishes fundamental connections between vision and language modalities<br>
                        • Trains only the MLP adapter while keeping other components frozen<br>
                        • Uses general-purpose data for robust cross-modal bridging</p>
                    </li>
                    <li>
                        <strong>Stage 1.5: Knowledge Injection</strong>
                        <p>• Massive injection of multimodal knowledge (300B VLM tokens + 30B text tokens)<br>
                        • Domain-specific enhancement for OCR, mathematics, and document understanding<br>
                        • Multi-task learning framework for comprehensive capability development</p>
                    </li>
                    <li>
                        <strong>Stage 2: Quality Refinement</strong>
                        <p>• High-quality instruction fine-tuning with 1B carefully curated tokens<br>
                        • Preference learning to reduce hallucinations<br>
                        • Integration of thinking data to enhance reasoning capabilities</p>
                    </li>
                </ol>

                <h3 id="architecture">3. Advanced Architecture Design</h3>
                <p>The Qianfan-VL architecture builds upon proven foundations while introducing key enhancements:</p>
                
                <div class="architecture-overview">
                    <img src="images/qianfan_vl_arch.png" alt="Qianfan-VL Architecture" class="architecture-image">
                </div>

                <div class="architecture-details">
                    <h4>Architecture Components</h4>
                    <ul>
                        <li><strong>Language Model:</strong> The language model adopts the Llama 3.1 architecture with an expanded vocabulary. It was obtained through vocabulary expansion and Chinese localization based on 3T Chinese-English corpus.</li>
                        <li><strong>Vision Encoder:</strong> The ViT employs dynamic tiling for processing images with dynamic resolution. We referenced InternVL's approach, and the Vision Encoder uses InternViT as initialization parameters for learning.</li>
                        <li><strong>Cross-Modal Fusion:</strong> MLP adapter bridges the vision and language modalities, enabling seamless multimodal understanding.</li>
                    </ul>
                </div>
            </section>

            <section id="domain-enhancement" class="section">
                <h2>Domain Enhancement in Action</h2>
                
                <h3>OCR and Document Understanding</h3>
                <p>
                    Qianfan-VL's domain enhancement shines particularly in OCR and document understanding tasks. 
                    The model employs a sophisticated data synthesis pipeline:
                </p>
                
                <div class="enhancement-grid">
                    <div class="enhancement-card">
                        <h4>Natural Scene Text Recognition</h4>
                        <p>Combining internet document images with traditional vision models and programmatic generation</p>
                    </div>
                    <div class="enhancement-card">
                        <h4>Structured Information Extraction</h4>
                        <p>Automated generation of table recognition and form extraction datasets</p>
                    </div>
                    <div class="enhancement-card">
                        <h4>Multi-scenario Coverage</h4>
                        <p>From natural scenes to office documents, handwritten text, and complex layouts</p>
                    </div>
                </div>

                <div class="performance-improvement">
                    <h4>Performance Improvements After Domain Enhancement</h4>
                    <table class="improvement-table">
                        <thead>
                            <tr>
                                <th>Task</th>
                                <th>Before</th>
                                <th>After</th>
                                <th>Improvement</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>DocOCR2Markdown</td>
                                <td>73.7%</td>
                                <td>75.0%</td>
                                <td class="positive">+1.3%</td>
                            </tr>
                            <tr>
                                <td>DocTable2HTML (Hard)</td>
                                <td>69.1%</td>
                                <td>68.4%</td>
                                <td class="neutral">-0.7%</td>
                            </tr>
                            <tr>
                                <td>Educational Tasks Average</td>
                                <td>80.09%</td>
                                <td>85.35%</td>
                                <td class="positive">+5.26%</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3>Mathematical Reasoning</h3>
                <p>The model introduces comprehensive mathematical task coverage through:</p>
                
                <ul class="math-features">
                    <li>Multi-language problem rewriting for broader understanding</li>
                    <li>Long Chain-of-Thought (LongCoT) synthesis for complex reasoning</li>
                    <li>Rejection sampling and filtering for quality control</li>
                    <li>Coverage of problem recognition, solving, analysis, and grading tasks</li>
                </ul>
                
                <p>
                    This results in superior performance on educational scenarios, with the domain-enhanced version achieving 
                    <strong>85.35% average accuracy</strong> across multiple mathematical tasks.
                </p>
            </section>

            <section id="training" class="section">
                <h2>Training at Scale with Kunlun Chips</h2>
                
                <p>
                    A remarkable achievement of Qianfan-VL is its complete training on Baidu's self-developed <strong>Kunlun P800 chips</strong>, 
                    representing a significant milestone in achieving AI infrastructure independence while maintaining world-class performance.
                </p>
                
                <div class="kunlun-features">
                    <div class="feature-card">
                        <h3>Training Infrastructure</h3>
                        <ul>
                            <li><strong>Massive Parallelism:</strong> 1k/2k/5k P800 cards for different model sizes</li>
                            <li><strong>End-to-End Training:</strong> All stages from pre-training to fine-tuning completed on P800</li>
                            <li><strong>Production Ready:</strong> Demonstrates the maturity of domestic AI infrastructure</li>
                        </ul>
                    </div>
                    
                    <div class="feature-card">
                        <h3>Training Data Scale</h3>
                        <ul>
                            <li><strong>Pre-training:</strong> 3.5T image-text tokens</li>
                            <li><strong>Knowledge Injection:</strong> 300B VLM tokens + 30B text tokens</li>
                            <li><strong>Fine-tuning:</strong> 1B carefully curated tokens</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section id="benchmarks" class="section">
                <h2>Performance Benchmarks</h2>
                
                <p>
                    Qianfan-VL demonstrates competitive performance across standard benchmarks while excelling in domain-specific tasks:
                </p>

                <h3>General Benchmarks</h3>
                <div class="table-wrapper">
                    <table class="benchmark-table">
                        <thead>
                            <tr>
                                <th>Benchmark/Model</th>
                                <th>Qianfan-VL-8B</th>
                                <th>Qianfan-VL-70B</th>
                                <th>Qwen2.5-VL-7B</th>
                                <th>Qwen2.5-VL-72B</th>
                                <th>InternVL3-8B</th>
                                <th>InternVL3-78B</th>
                                <th>Claude-3.5-Sonnet</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>SEEDBench_IMG</td>
                                <td>77.82</td>
                                <td><strong>78.77</strong></td>
                                <td>76.98</td>
                                <td><strong>78.34</strong></td>
                                <td>77.00</td>
                                <td>77.52</td>
                                <td>71.18</td>
                            </tr>
                            <tr>
                                <td>AI2D_TEST</td>
                                <td>84.62</td>
                                <td><strong>86.14</strong></td>
                                <td>80.47</td>
                                <td>83.84</td>
                                <td><strong>85.07</strong></td>
                                <td>83.55</td>
                                <td>60.75</td>
                            </tr>
                            <tr>
                                <td>ChartQA_TEST</td>
                                <td>87.96</td>
                                <td><strong>88.44</strong></td>
                                <td>86.68</td>
                                <td><strong>87.16</strong></td>
                                <td>85.76</td>
                                <td>82.04</td>
                                <td>23.96</td>
                            </tr>
                            <tr>
                                <td>MathVista_MINI</td>
                                <td>66.70</td>
                                <td><strong>74.00</strong></td>
                                <td>67.50</td>
                                <td><strong>74.30</strong></td>
                                <td>69.60</td>
                                <td>69.40</td>
                                <td>72.30</td>
                            </tr>
                            <tr>
                                <td>MMMU_VAL</td>
                                <td><strong>70.67</strong></td>
                                <td><strong>71.78</strong></td>
                                <td>51.00</td>
                                <td>65.78</td>
                                <td>56.11</td>
                                <td>60.78</td>
                                <td>52.33</td>
                            </tr>
                            <tr>
                                <td>MMT-Bench_VAL</td>
                                <td>63.61</td>
                                <td><strong>71.38</strong></td>
                                <td>61.40</td>
                                <td><strong>69.49</strong></td>
                                <td>65.17</td>
                                <td>63.67</td>
                                <td>53.95</td>
                            </tr>
                            <tr>
                                <td>MMStar</td>
                                <td>65.73</td>
                                <td><strong>70.93</strong></td>
                                <td>61.53</td>
                                <td>66.00</td>
                                <td><strong>68.40</strong></td>
                                <td>66.07</td>
                                <td>51.53</td>
                            </tr>
                            <tr>
                                <td>OCRBench</td>
                                <td>850</td>
                                <td>846</td>
                                <td><strong>883</strong></td>
                                <td>874</td>
                                <td><strong>881</strong></td>
                                <td>847</td>
                                <td>731</td>
                            </tr>
                            <tr>
                                <td>OCRVQA_TESTCORE</td>
                                <td>69.08</td>
                                <td><strong>72.79</strong></td>
                                <td><strong>71.02</strong></td>
                                <td>66.80</td>
                                <td>39.03</td>
                                <td>35.58</td>
                                <td>26.79</td>
                            </tr>
                            <tr>
                                <td>TextVQA_VAL</td>
                                <td>82.87</td>
                                <td>81.84</td>
                                <td><strong>84.96</strong></td>
                                <td>83.26</td>
                                <td>82.15</td>
                                <td>83.52</td>
                                <td>61.20</td>
                            </tr>
                            <tr>
                                <td>InfoVQA_VAL</td>
                                <td>76.48</td>
                                <td>76.70</td>
                                <td>82.45</td>
                                <td><strong>85.46</strong></td>
                                <td>75.84</td>
                                <td>77.58</td>
                                <td>43.96</td>
                            </tr>
                            <tr>
                                <td>DocVQA</td>
                                <td>93.83</td>
                                <td>93.40</td>
                                <td>94.91</td>
                                <td><strong>95.75</strong></td>
                                <td>92.04</td>
                                <td>83.82</td>
                                <td>78.75</td>
                            </tr>
                            <tr>
                                <td>CharXiv_DQ</td>
                                <td><strong>95.22</strong></td>
                                <td><strong>96.20</strong></td>
                                <td>78.60</td>
                                <td>10.00</td>
                                <td>75.70</td>
                                <td>84.90</td>
                                <td>88.32</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                
                <h3>Domain-Specific Performance</h3>
                <p>The domain-enhanced version shows significant improvements over the general version:</p>
                
                <ul class="performance-list">
                    <li>Educational task average: 80.09% → 85.35% (+5.26%)</li>
                    <li>Document understanding tasks show 5-15% improvement</li>
                    <li>Superior performance compared to both general VL models and specialized models</li>
                    <li>OCR accuracy significantly outperforms general-purpose VL models</li>
                </ul>
            </section>

            <section id="applications" class="section">
                <h2 id="model-capabilities">Model Capabilities</h2>
                
                <div class="capabilities-container">
                    <!-- Tab Navigation -->
                    <div class="capability-tabs">
                        <button class="capability-tab active" data-tab="ocr">OCR</button>
                        <button class="capability-tab" data-tab="math">Math Problem Solving</button>
                        <button class="capability-tab" data-tab="document">Document Understanding</button>
                        <button class="capability-tab" data-tab="video">Video Understanding</button>
                        <button class="capability-tab" data-tab="grounding">Grounding & Counting</button>
                    </div>

                    <!-- Tab Content -->
                    <div class="capability-panels">
                        <!-- OCR Panel -->
                        <div class="capability-panel active" id="ocr-panel">
                            <div class="carousel-container">
                                <button class="carousel-btn carousel-prev" aria-label="Previous">‹</button>
                                <div class="carousel-wrapper">
                                    <div class="carousel-track">
                                        <!-- Handwritten OCR -->
                                        <div class="capability-card">
                                            <h4>Handwritten Recognition</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/handwritten_recognition.png" alt="Handwritten text example">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> 识别图中的文字
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> 征衣未解再跨鞍，接续奋斗开新局。中国青年作为中国大厦的不朽钢钉，中国革命的燎原火种，中国故事的青春注脚，接过时代的话筒，当有对答世界的音量，让中国声音燎亮东方。奋斗圆梦，再谱华章！
                                    </div>
                                </div>
                            </div>
                                        </div>

                                        <!-- Scene OCR -->
                                        <div class="capability-card">
                                            <h4>Scene OCR</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/scene_ocr.png" alt="Street scene with text">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> 请识别图中文字
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> 实景现房中赫出品 <br>
                                            颐和园公园南门800米/北侧距万柳高尔夫80米 <br>
                                            心30米/距地铁巴沟站C出口10米 <br>
                                            T·6271
                                    </div>
                                </div>
                            </div>
                                        </div>

                                        <!-- Math Equation Recognition -->
                                        <div class="capability-card">
                                            <h4>Math Equation Recognition</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/math_equation_recognition.jpg" alt="Mathematical equations">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> 将图中的公式转换为latex
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> <code>\begin{align*}r_d(G):=\sum_{F\in\mathcal{F}_G^+}\frac{R_d(F)}{d_F^2},\end{align*}</code>
                                    </div>
                                </div>
                            </div>
                                        </div>

                                        <!-- Information Extraction -->
                                        <div class="capability-card">
                                            <h4>Information Extraction</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/information_extraction.jpeg" alt="Business form">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> 提取发票信息：发票号码、开票日期、总金额、项目列表等
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> <pre>{"发票号码": "0110020001", "开票日期": "2021年10月", "合计": "1565.05", "价税合计": "1580.70", "项目列表": [{"项目名称": "成药*安宫牛黄丸", "数量": "1", "金额": "749.11"}, {"项目名称": "成药*益安宁丸", "数量": "1", "金额": "758.71"}, {"项目名称": "学药品制剂*藿香正气软胶囊", "数量": "2", "金额": "57.23"}]}</pre>
                                    </div>
                                </div>
                            </div>
                                        </div>
                                    </div>
                                </div>
                                <button class="carousel-btn carousel-next" aria-label="Next">›</button>
                            </div>
                        </div>

                        <!-- Math Problem Solving Panel -->
                        <div class="capability-panel" id="math-panel">
                            <div class="carousel-container">
                                <button class="carousel-btn carousel-prev" aria-label="Previous">‹</button>
                                <div class="carousel-wrapper">
                                    <div class="carousel-track">
                                        <!-- Photo Search Math Problem -->
                                        <div class="capability-card">
                                            <h4>Photo Search Math Problem Solving</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="placeholder_math_photo.jpg" alt="Math problem photo">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> Solve this math problem and show the steps.
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> [Provides step-by-step solution with clear explanations]
                                    </div>
                                </div>
                            </div>
                                        </div>

                                        <!-- Geometry Problem -->
                                        <div class="capability-card">
                                            <h4>Geometry Problem Solving</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/geometry_problem_solving.jpg" alt="Geometry diagram">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> 请帮我解一下这道题
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> 要解决在△ABC中，∠B=2∠C，AD⊥BC于D，M为BC中点，AB=10，求DM长度的问题，可按以下步骤进行：
                                            ### **关键思路**  
                                            1. **利用角关系与正弦定理**：通过∠B=2∠C，结合正弦定理表示边与角的关系。  
                                            2. **直角三角形斜边中线性质**：在Rt△ABD中，取AB中点E，连接DE，则DE为斜边AB的中线，故DE=BE=AE=5。  
                                            3. **中点构造与中位线**：M为BC中点，结合辅助线构造平行四边形或等腰三角形，转化线段关系。  
                                            
                                            ### **详细步骤**  
                                            设∠C=θ，则∠B=2θ，∠A=180°-3θ。  
                                            - **在Rt△ABD中**：AD⊥BC，故∠ADB=90°。由正弦定理：  
                                              \( \frac{AB}{\sin\theta} = \frac{AC}{\sin2\theta} \)，得 \( AC = \frac{AB \cdot \sin2\theta}{\sin\theta} = 20\cos\theta \)（因为 \( \sin2\theta=2\sin\theta\cos\theta \)）。  
                                            - **求BD与DC**：  
                                              在Rt△ABD中，\( BD = AB \cdot \cos2\theta = 10\cos2\theta \)；  
                                              在Rt△ACD中，\( DC = AC \cdot \cos\theta = 20\cos^2\theta \)。  
                                            - **求BC与BM**：  
                                              \( BC = BD + DC = 10\cos2\theta + 20\cos^2\theta \)。  
                                              因为 \( \cos2\theta = 2\cos^2\theta - 1 \)，代入得：  
                                              \( BC = 10(2\cos^2\theta - 1) + 20\cos^2\theta = 40\cos^2\theta - 10 \)。  
                                              M为BC中点，故 \( BM = \frac{BC}{2} = 20\cos^2\theta - 5 \)。  
                                            
                                            ### **求DM**  
                                            由 \( BD = 10\cos2\theta = 20\cos^2\theta - 10 \)，则：  
                                            \( DM = BM - BD = (20\cos^2\theta - 5) - (20\cos^2\theta - 10) = 5 \)。  
                                            
                                            ### **结论**  
                                            **DM的长度为5**。
                                            
                                            **答案**：\(\boxed{5}\)
                                    </div>
                                </div>
                            </div>
                                        </div>
                                    </div>
                                </div>
                                <button class="carousel-btn carousel-next" aria-label="Next">›</button>
                            </div>
                        </div>

                        <!-- Document Understanding Panel -->
                        <div class="capability-panel" id="document-panel">
                            <div class="carousel-container">
                                <button class="carousel-btn carousel-prev" aria-label="Previous">‹</button>
                                <div class="carousel-wrapper">
                                    <div class="carousel-track">
                                        <!-- Document Content Understanding -->
                                        <div class="capability-card">
                                            <h4>Document Content Understanding</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/document_content_understanding.png" alt="Complex document">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> 根据文档内容，中国银监会在文件中强调了对商业性房地产信贷的哪些具体管理要求？这些要求对银行的风险控制有何影响？
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> 文件提出了四项核心要求:1）严格房地产开发贷款管理，限制资金流向未合规项目；2）规范土地储备贷款，限制贷款比例和期限；3）加强住房消费贷款管理，实施差异化首付比例和利率；4）严控商业用房贷款。这些要求通过限制资金过度流入房地产、提高贷款门槛和差异化定价，有效降低了银行信贷风险，防止了房地产泡沫。
                                    </div>
                                </div>
                            </div>
                                        </div>

                                        <!-- Table Understanding -->
                                        <div class="capability-card">
                                            <h4>Table Understanding</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/table_understanding.png" alt="Complex table">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> 将表格解析为HTML格式，保留合并单元格结构
                                    </div>
                                    <div class="model-response">
                                        
                                        <table border="1">
                                            <thead>
                                                <tr>
                                                    <th>项目</th> <th>期末余额</th> <th>年初余额</th>
                                                </tr>
                                            </thead>
                                            <tbody>
                                                <tr>  <td colspan="3">流动资产：</td>  </tr>
                                                <tr>  <td>货币资金</td> <td>5,288,011,738.39</td> <td>2,629,981,089.49</td>  </tr>
                                                <tr>  <td>结算备付金</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>拆出资金</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>以公允价值计量且其变动计入当期损益的金融资产</td> <td>3,226,301.37</td> <td>6,830,273.97</td>  </tr>
                                                <tr>  <td>衍生金融资产</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>应收票据</td> <td>1,010,636.94</td> <td>888,800.00</td>  </tr>
                                                <tr>  <td>应收账款</td> <td>389,409,537.65</td> <td>290,733,812.62</td>  </tr>
                                                <tr>  <td>预付款项</td> <td>474,301,533.93</td> <td>509,510,514.92</td>  </tr>
                                                <tr>  <td>应收保费</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>应收分保账款</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>应收分保合同准备金</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>应收利息</td> <td>8,746,899.35</td> <td>1,645,890.39</td>  </tr>
                                                <tr>  <td>应收股利</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>其他应收款</td> <td>291,499,926.83</td> <td>364,426,205.02</td>  </tr>
                                                <tr>  <td>买入返售金融资产</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>存货</td> <td>69,558,687.59</td> <td>79,651,862.50</td>  </tr>
                                                <tr>  <td>划分为持有待售的资产</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>一年内到期的非流动资产</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>其他流动资产</td> <td>533,938,653.95</td> <td>545,184,036.20</td>  </tr>
                                                <tr>  <td>流动资产合计</td> <td>7,059,703,916.00</td> <td>4,428,852,485.11</td>  </tr>
                                                <tr>  <td colspan="3">非流动资产：</td>  </tr>
                                                <tr>  <td>发放贷款和垫款</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>可供出售金融资产</td> <td>981,477,845.00</td> <td>42,732,350.00</td>  </tr>
                                                <tr>  <td>持有至到期投资</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>长期应收款</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>长期股权投资</td> <td>478,328,423.92</td> <td>482,914,333.17</td>  </tr>
                                                <tr>  <td>投资性房地产</td> <td></td> <td></td>  </tr>
                                                <tr>  <td>固定资产</td> <td>11,211,345,599.37</td> <td>11,517,509,788.66</td>  </tr>
                                                <tr>  <td>在建工程</td> <td>344,034,065.45</td> <td>558,797,792.68</td>  </tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </div>
                            </div>
                        </div>

                                        <!-- Chart Reasoning -->
                                        <div class="capability-card">
                                            <h4>Chart Reasoning</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/chart_gift_reasoning.jpeg" alt="Gift selection chart">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> 小明是一位男生，下面哪样物品最适合作为他的七夕节礼物 a) Iphone 17 b) Dior香水 c) 一束美丽的鲜花 d) 巧克力套装
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> 根据图表数据，男生最想收到的礼物中，手机的热度指数为44.36，高于香水（47.62）、巧克力套装（42.48）和鲜花（41.22）。

选项分析：
- a) iphone17：属于手机类，符合男生对电子产品的偏好，热度指数在男生礼物中处于较高水平（仅次于游戏设备）。
- b) Dior香水：女生对香水的热度（87.13）远高于男生（47.62），且男生对香水的兴趣较低。
- c) 一束美丽的鲜花：男生对花的兴趣明显低于女生（男生花的热度41.22），通常不被视为男生偏好的礼物。
- d) 巧克力套装：女生对巧克力的热度（42.48）高于男生（47.62），且巧克力作为礼物更偏向"大众化"，而非男生核心兴趣。

最适合的礼物是a) iphone17。
答案：a
                                    </div>
                                </div>
                            </div>
                                        </div>

                                        <!-- Chart Calculation -->
                                        <div class="capability-card">
                                            <h4>Chart Calculation</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/chart_math_calculation.png" alt="AI sentiment chart">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> What is the average percentage of U.S. adults who feel equally excited and concerned about AI over the three years?
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> To determine the average percentage of U.S. adults who feel "equally excited and concerned" about AI over the three years (2021, 2022, 2023), follow these steps:

Step 1: Identify the relevant data
From the chart, the percentage of adults who feel "equally excited and concerned" is:
- 2021: 45%
- 2022: 46%
- 2023: 36%

Step 2: Calculate the average
Add the percentages and divide by the number of years (3):

Average = (45 + 46 + 36) / 3 = 127 / 3 ≈ 42.33%

Answer: Approximately 42.3% (or 42.33%).
                                    </div>
                                </div>
                            </div>
                                        </div>

                                        <!-- Stock Chart Analysis -->
                                        <div class="capability-card">
                                            <h4>Stock Chart Analysis</h4>
                            <div class="showcase-content">
                                <div class="showcase-image">
                                    <img src="images/chart_stock_elaboration.png" alt="Shanghai stock index chart">
                                </div>
                                <div class="showcase-dialogue">
                                    <div class="user-prompt">
                                        <strong>User:</strong> 描述一下8月27号的上证指数走势
                                    </div>
                                    <div class="model-response">
                                        <strong>Qianfan-VL:</strong> 8月27日，上证指数全天呈现震荡下跌走势。开盘后指数在3869.61点起步，最高触及3887.20点，随后逐步回落。早盘维持震荡整理态势，午市后跌幅逐渐扩大，最终收于3800.35点，下跌68.03点，跌幅1.76%。全天成交量达8.59亿手，成交额为1.33万亿元，市场交投活跃。从分时图看，指数在午后14:30左右出现明显跳水，跌幅进一步扩大，最终以全日低点收盘。
                                    </div>
                                </div>
                            </div>
                                        </div>
                                    </div>
                                </div>
                                <button class="carousel-btn carousel-next" aria-label="Next">›</button>
                            </div>
                        </div>

                        <!-- Video Understanding Panel -->
                        <div class="capability-panel" id="video-panel">
                            <div class="carousel-container">
                                <button class="carousel-btn carousel-prev" aria-label="Previous">‹</button>
                                <div class="carousel-wrapper">
                                    <div class="carousel-track">
                                        <!-- Video Content Analysis -->
                                        <div class="capability-card">
                                            <h4>Video Content Analysis</h4>
                                            <div class="showcase-content">
                                                <div class="showcase-image">
                                                    <video controls style="width: 100%;">
                                                        <source src="images/video_understanding.mp4" type="video/mp4">
                                                        Your browser does not support the video tag.
                                                    </video>
                                                </div>
                                                <div class="showcase-dialogue">
                                                    <div class="user-prompt">
                                                        <strong>User:</strong> 请描述一下视频中的内容
                                                    </div>
                                                    <div class="model-response">
                                                        <strong>Qianfan-VL:</strong> 在这段宁静祥和的视频中，一片宽阔的湖泊映入眼帘，湖面波光粼粼，反射着温暖的阳光。几艘小船散布在湖面上，有的靠近岸边，有的在湖心悠然行驶。远处，茂密的树林和连绵起伏的山脉构成了一幅美丽的画卷…
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <button class="carousel-btn carousel-next" aria-label="Next">›</button>
                            </div>
                        </div>

                        <!-- Grounding and Counting Panel -->
                        <div class="capability-panel" id="grounding-panel">
                            <div class="carousel-container">
                                <button class="carousel-btn carousel-prev" aria-label="Previous">‹</button>
                                <div class="carousel-wrapper">
                                    <div class="carousel-track">
                                        <!-- Object Grounding -->
                                        <div class="capability-card">
                                            <h4>Object Grounding</h4>
                                            <div class="showcase-content">
                                                <div class="showcase-image">
                                                    <img src="images/object_grounding_result.jpg" alt="Animals with cat localization">
                                                </div>
                                                <div class="showcase-dialogue">
                                                    <div class="user-prompt">
                                                        <strong>User:</strong> Please provide the bounding box coordinate of the region this sentence describes: &lt;ref&gt;cat in the middle&lt;/ref&gt;
                                                    </div>
                                                    <div class="model-response">
                                                        <strong>Qianfan-VL:</strong> &lt;ref&gt;cat in the middle&lt;/ref&gt;&lt;box&gt;[[416, 577, 694, 804]]&lt;/box&gt;
                                                    </div>
                                                </div>
                                            </div>
                                        </div>

                                        <!-- Object Counting -->
                                        <div class="capability-card">
                                            <h4>Object Counting</h4>
                                            <div class="showcase-content">
                                                <div class="showcase-image">
                                                    <img src="images/object_counting.jpg" alt="Multiple watermelons">
                                                </div>
                                                <div class="showcase-dialogue">
                                                    <div class="user-prompt">
                                                        <strong>User:</strong> 图中有几个西瓜
                                                    </div>
                                                    <div class="model-response">
                                                        <strong>Qianfan-VL:</strong> 10
                                                    </div>
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                                <button class="carousel-btn carousel-next" aria-label="Next">›</button>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section id="api" class="section">
                <h2>Getting Started</h2>
                
                <h3>Installation</h3>
                <pre><code class="language-bash"># Install OpenAI SDK
pip install openai

# Set up authentication (using Qianfan credentials)
export QIANFAN_ACCESS_KEY="your_access_key"
export QIANFAN_SECRET_KEY="your_secret_key"</code></pre>

                <h3>Basic Setup</h3>
                <pre><code class="language-python">from openai import OpenAI
import base64
import os

# Initialize client with Qianfan endpoint
client = OpenAI(
    api_key = os.environ.get("QIANFAN_SECRET_KEY"),
    base_url = "https://api.qianfan.com/v1",
    default_headers = {
        "X-Access-Key": os.environ.get("QIANFAN_ACCESS_KEY")
    }
)

# Available models: Qianfan-VL-8B, Qianfan-VL-70B
# Note: Qianfan-VL-3B will be available soon
model = "Qianfan-VL-8B"</code></pre>

                <h3>Code Examples by Capability</h3>
                
                <div class="code-examples-container">
                    <!-- Code Example Tabs -->
                    <div class="code-tabs">
                        <button class="code-tab active" data-tab="ocr-code">OCR</button>
                        <button class="code-tab" data-tab="math-code">Math Solving</button>
                        <button class="code-tab" data-tab="document-code">Document Understanding</button>
                        <button class="code-tab" data-tab="video-code">Video Analysis</button>
                        <button class="code-tab" data-tab="grounding-code">Grounding & Counting</button>
                    </div>

                    <!-- Code Panels -->
                    <div class="code-panels">
                        <!-- OCR Code Panel -->
                        <div class="code-panel active" id="ocr-code-panel">
                            <h4>OCR and Text Extraction</h4>
                            <pre><code class="language-python">from openai import OpenAI
import base64
import os

client = OpenAI(
    api_key = os.environ.get("QIANFAN_SECRET_KEY"),
    base_url = "https://api.qianfan.com/v1",
    default_headers = {"X-Access-Key": os.environ.get("QIANFAN_ACCESS_KEY")}
)

# Read and encode image
with open("handwritten_text.jpg", "rb") as image_file:
    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')

# OCR for handwritten text
response = client.chat.completions.create(
    model = "Qianfan-VL-8B",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "请识别图中的手写文字，并转换为简体中文"},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ],
    temperature = 0.1,  # Lower temperature for accurate OCR
    max_tokens = 2048
)

print(response.choices[0].message.content)

# Scene text extraction
scene_response = client.chat.completions.create(
    model = "Qianfan-VL-8B",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "提取图片中所有的文字信息，包括招牌、标语等"},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ]
)

# Extract structured information from forms
form_response = client.chat.completions.create(
    model = "Qianfan-VL-8B",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "提取表单中的关键信息，输出为JSON格式"},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ]
)</code></pre>
                        </div>

                        <!-- Math Code Panel -->
                        <div class="code-panel" id="math-code-panel">
                            <h4>Mathematical Problem Solving</h4>
                            <pre><code class="language-python">from openai import OpenAI
import base64
import os

client = OpenAI(
    api_key = os.environ.get("QIANFAN_SECRET_KEY"),
    base_url = "https://api.qianfan.com/v1",
    default_headers = {"X-Access-Key": os.environ.get("QIANFAN_ACCESS_KEY")}
)

# Read math problem image
with open("math_problem.jpg", "rb") as image_file:
    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')

# Solve with step-by-step explanation
response = client.chat.completions.create(
    model = "Qianfan-VL-70B",  # Use larger model for complex math
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text", 
                    "text": """请解决这道数学题，要求：
1. 详细说明解题思路
2. 给出每一步的推导过程
3. 使用LaTeX格式书写数学公式
4. 最后给出答案"""
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ],
    temperature = 0.2,
    max_tokens = 4096
)

print(response.choices[0].message.content)

# Geometry problem solving
geometry_response = client.chat.completions.create(
    model = "Qianfan-VL-70B",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "分析这道几何题，画出辅助线并说明解题步骤"
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ]
)

# Extract mathematical equations
equation_response = client.chat.completions.create(
    model = "Qianfan-VL-8B",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "将图中的数学公式转换为LaTeX格式"},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ]
)</code></pre>
                        </div>

                        <!-- Document Code Panel -->
                        <div class="code-panel" id="document-code-panel">
                            <h4>Document Understanding</h4>
                            <pre><code class="language-python">from openai import OpenAI
import base64
import os

client = OpenAI(
    api_key = os.environ.get("QIANFAN_SECRET_KEY"),
    base_url = "https://api.qianfan.com/v1",
    default_headers = {"X-Access-Key": os.environ.get("QIANFAN_ACCESS_KEY")}
)

# Read document image
with open("document.jpg", "rb") as image_file:
    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')

# Extract and analyze document content
response = client.chat.completions.create(
    model = "Qianfan-VL-8B",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": """请分析这份文档：
1. 提取文档的主要内容和关键信息
2. 识别文档类型（合同、报告、财务报表等）
3. 总结核心要点
4. 标注重要条款或数据"""
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ],
    temperature = 0.1,
    max_tokens = 3000
)

print(response.choices[0].message.content)

# Table extraction and analysis
table_response = client.chat.completions.create(
    model = "Qianfan-VL-8B",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": """请提取表格内容：
1. 将表格转换为Markdown格式
2. 保留所有合并单元格的结构
3. 计算关键指标（如有）
4. 分析数据趋势"""
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ]
)

# Chart analysis
chart_response = client.chat.completions.create(
    model = "Qianfan-VL-70B",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "分析这个图表，提取数据并给出洞察"
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ]
)</code></pre>
                        </div>

                        <!-- Video Code Panel -->
                        <div class="code-panel" id="video-code-panel">
                            <h4>Video Understanding</h4>
                            <pre><code class="language-python">from openai import OpenAI
import base64
import os
import cv2

client = OpenAI(
    api_key = os.environ.get("QIANFAN_SECRET_KEY"),
    base_url = "https://api.qianfan.com/v1",
    default_headers = {"X-Access-Key": os.environ.get("QIANFAN_ACCESS_KEY")}
)

# Extract key frames from video
def extract_frames(video_path, num_frames = 8):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_indices = [i * total_frames // num_frames for i in range(num_frames)]
    
    frames = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            _, buffer = cv2.imencode('.jpg', frame)
            frame_base64 = base64.b64encode(buffer).decode('utf-8')
            frames.append(frame_base64)
    
    cap.release()
    return frames

# Extract frames
frames = extract_frames("video.mp4", num_frames = 8)

# Analyze video content
content = []
for i, frame in enumerate(frames):
    content.append({"type": "text", "text": f"Frame {i+1}:"})
    content.append({
        "type": "image_url",
        "image_url": {"url": f"data:image/jpeg;base64,{frame}"}
    })

content.append({"type": "text", "text": """请分析这个视频：
1. 描述视频的整体内容
2. 识别主要场景和活动
3. 提取关键时刻
4. 总结视频的主题"""})

response = client.chat.completions.create(
    model = "Qianfan-VL-70B",
    messages=[{"role": "user", "content": content}],
    temperature = 0.3,
    max_tokens = 2048
)

print(response.choices[0].message.content)

# Action recognition
action_response = client.chat.completions.create(
    model = "Qianfan-VL-8B",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "识别视频中的动作和行为"},
                *[{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{f}"}} for f in frames[:4]]
            ]
        }
    ]
)</code></pre>
                        </div>

                        <!-- Grounding Code Panel -->
                        <div class="code-panel" id="grounding-code-panel">
                            <h4>Grounding and Counting</h4>
                            <pre><code class="language-python">from openai import OpenAI
import base64
import os
import json

client = OpenAI(
    api_key = os.environ.get("QIANFAN_SECRET_KEY"),
    base_url = "https://api.qianfan.com/v1",
    default_headers = {"X-Access-Key": os.environ.get("QIANFAN_ACCESS_KEY")}
)

# Read image
with open("objects.jpg", "rb") as image_file:
    image_base64 = base64.b64encode(image_file.read()).decode('utf-8')

# Object grounding with bounding boxes
grounding_response = client.chat.completions.create(
    model = "Qianfan-VL-8B",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "Please provide the bounding box coordinates for: <ref>the red car in the middle</ref>"
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ],
    temperature = 0.1,
    max_tokens = 512
)

# Parse bounding box from response
response_text = grounding_response.choices[0].message.content
print(f"Grounding result: {response_text}")

# Object counting
counting_response = client.chat.completions.create(
    model = "Qianfan-VL-8B",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "请数一下图中有多少个人、多少辆车、多少棵树"},
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ],
    temperature = 0.1,
    max_tokens = 256
)

print(f"Counting result: {counting_response.choices[0].message.content}")

# Detailed object detection
detection_response = client.chat.completions.create(
    model = "Qianfan-VL-70B",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": """请检测图中所有对象，输出JSON格式：
{
  "objects": [
    {"name": "对象名称", "count": 数量, "location": "位置描述"},
    ...
  ]
}"""
                },
                {
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}
                }
            ]
        }
    ],
    temperature = 0.1,
    max_tokens = 1024
)

# Parse JSON response
try:
    objects = json.loads(detection_response.choices[0].message.content)
    print(json.dumps(objects, ensure_ascii = False, indent = 2))
except json.JSONDecodeError:
    print(detection_response.choices[0].message.content)</code></pre>
                        </div>
                    </div>
                </div>

                <h3>API Parameters</h3>
                <table class="api-table">
                    <thead>
                        <tr>
                            <th>Parameter</th>
                            <th>Type</th>
                            <th>Default</th>
                            <th>Description</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><code>model</code></td>
                            <td>string</td>
                            <td>required</td>
                            <td>Available models: "Qianfan-VL-8B", "Qianfan-VL-70B" (3B coming soon)</td>
                        </tr>
                        <tr>
                            <td><code>messages</code></td>
                            <td>array</td>
                            <td>required</td>
                            <td>Array of message objects with role and content</td>
                        </tr>
                        <tr>
                            <td><code>temperature</code></td>
                            <td>float</td>
                            <td>0.7</td>
                            <td>Sampling temperature (0.0 to 2.0)</td>
                        </tr>
                        <tr>
                            <td><code>max_tokens</code></td>
                            <td>integer</td>
                            <td>2048</td>
                            <td>Maximum tokens to generate</td>
                        </tr>
                        <tr>
                            <td><code>top_p</code></td>
                            <td>float</td>
                            <td>1.0</td>
                            <td>Nucleus sampling parameter</td>
                        </tr>
                        <tr>
                            <td><code>stream</code></td>
                            <td>boolean</td>
                            <td>false</td>
                            <td>Enable streaming responses</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section id="conclusion" class="section">
                <h2>Conclusion</h2>
                
                <p>
                    Qianfan-VL represents a significant step forward in making multimodal AI practical for enterprise applications. 
                    By combining general capabilities with domain-specific enhancements, efficient training on domestic hardware, 
                    and a comprehensive model family, Qianfan-VL offers a compelling solution for organizations looking to leverage multimodal AI.
                </p>
                
                <p>
                    The model's success in bridging the gap between research and production, particularly in challenging domains like OCR 
                    and document understanding, demonstrates the value of targeted enhancement while maintaining broad capabilities. 
                    As multimodal understanding becomes essential for enterprise intelligence, Qianfan-VL provides a robust foundation 
                    for the next generation of AI applications.
                </p>

                <div class="cta-section">
                    <h3>Get Started with Qianfan-VL</h3>
                    <p>Experience the power of domain-enhanced multimodal understanding</p>
                    <a href="https://console.bce.baidu.com/qianfan" class="btn btn-primary">Access Console</a>
                    <a href="https://cloud.baidu.com/doc/WENXINWORKSHOP" class="btn btn-secondary">Read Documentation</a>
                    <a href="https://github.com/baidubce/qianfan-models-cookbook" class="btn btn-secondary">View Examples</a>
                </div>
            </section>

            <section id="resources" class="section">
                <h2>Resources & Support</h2>
                
                <div class="resources-grid">
                    <div class="resource-card">
                        <h3>Documentation</h3>
                        <ul>
                            <li><a href="https://cloud.baidu.com/doc/WENXINWORKSHOP">API Documentation</a></li>
                            <li><a href="https://github.com/baidubce/qianfan-models-cookbook">Code Examples</a></li>
                            <li><a href="https://console.bce.baidu.com/qianfan">Console</a></li>
                        </ul>
                    </div>
                    
                    <div class="resource-card">
                        <h3>Community</h3>
                        <ul>
                            <li><a href="https://ai.baidu.com/forum">Discussion Forum</a></li>
                            <li><a href="https://github.com/baidubce">GitHub Organization</a></li>
                            <li>WeChat: 百度智能云千帆</li>
                        </ul>
                    </div>
                    
                    <div class="resource-card">
                        <h3>Contact</h3>
                        <ul>
                            <li>Technical: qianfan-support@baidu.com</li>
                            <li>Business: qianfan-biz@baidu.com</li>
                            <li>Research: qianfan-research@baidu.com</li>
                        </ul>
                    </div>
                </div>
            </section>
        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-info">
                    <p>&copy; 2025 Baidu, Inc. All rights reserved.</p>
                    <p>Qianfan-VL is a product of Baidu AI Cloud.</p>
                </div>
                <div class="footer-links">
                    <a href="https://cloud.baidu.com">Baidu Cloud</a>
                    <a href="https://ai.baidu.com">Baidu AI</a>
                    <a href="https://ir.baidu.com">Investor Relations</a>
                    <a href="https://www.baidu.com/duty">Legal</a>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
</body>
</html>
